---
title: "R GIS"
output: 
  html_document:
    toc: true
    toc_float:
      toc_collapsed: true
    toc_depth: 2
    theme: cerule
---

# Tonight's Goals

- Reproducibility
- Random Forests
- Lab: Choose your own adventure



# Reproducibility

Morgan shared an Excel file with me, which she has agreed to let me use in class. Now I will awkwardly put her on the spot and ask her to tell us what they were trying to do with this Excel file. See [Track and Field National Prep.xlsx](data/Track and Field National Prep.xlsx) in the data folder to see what I am talking about.


1. The original worksheets include both data entry AND calculation.
2. Unfortunately, reordering rows of data in Excel is hard.
3. So, we will just do it in R.

- **Problem:** Reproducing this report with a new data set would be time consuming and error prone. (Disclaimer, all data entry is error prone.) Reproducing
- **Goal:** Make the report reproducible, without having to manually move the data around or calculate points.

## Next Steps

- We moved the data into two new worksheets.
    - Yes, this was done manually.
    - But it creates a process of "pure" data entry.
- Separation of Concerns
- See [track-and-field-plan.Rmd](track-and-field-plan.Rmd) in this week's folder.

This script should look familiar to you.



# Random Forests

```{r setup}
## Make sure you run this, it loads a couple of additional packages.
library(modelr)
library(randomForest)
library(rpart)
library(rpart.plot)
library(tidyverse)

titanic <- read_csv("data/titanic-train-clean.csv") %>%
  mutate(
    ## Remember, we have to make pclass into a character column.
    pclass = as.character(pclass),
    died = !survived,
  ) %>%
  select(-survived)
```

## Our Titanic Data

```{r}
titanic
```

## A BIG Decision Tree

- Let's rebuild one of our last CaRT models from last week.
- CaRT: Classification and Regresstion Tree
- Yeah, the metaphor is a stretch. I know.

```{r}
cart_model <- rpart(died~sex+pclass+age+embarked, data = titanic, method = "class")
summary(cart_model)
rpart.plot(cart_model)
```

And we can assess how good our model is:

```{r}
titanic$cart_model <-
  predict(cart_model, newdata = titanic %>% mutate(died = NA))[,2]

titanic <- 
  titanic %>%
  mutate(
    ## Using a risk of .5 as the cut point is the same as using a odds of > 1.
    predicted_cart_model = case_when(cart_model > .5~1, TRUE~0)
  )

titanic %>%
  group_by(died) %>%
  summarize(
    predicted_died = sum(if_else(predicted_cart_model == 1, 1, 0)),
    predicted_survived = sum(if_else(predicted_cart_model == 0, 1, 0))
  )
```

- Accuracy: `r (513+218) / 891` 82.04%
- Two more ways to assess a classification model:
    - Sensitivity: `r 513 / (513 + 124)` 80.53%
    - Specificity: `r 218 / (218 + 36)` 85.83%



# From One Tree, To Many

- Thus far, our CaRT models have been of a single tree.
- And single trees can be over fit (and we talked about some ways to control for this).
- But there is one even more interesting way to control for overfitting.
- Instead of building one tree, build 1,000 (or more)!
- [Wikipedia: Random Forests](https://en.wikipedia.org/wiki/Random_forest)

But first I need to introduce bagging, which is basically an applied form of the bootstrap.

```{r}
## What is the average age of a passenger?
titanic %>%
  summarize(
    avg_age = mean(age, na.rm = TRUE)
  )
```
Previously, we dicussed an idea called Monte Carlo which allows us to infer the distribution of data using a sample, by sampling it a bunch of times. Let's review this briefly (this is not on the final).

```{r}
## The one-thousand voyages of Titanic!
repetitions <- 1000
average_age <- numeric(repetitions)
for (i in 1:repetitions) {
  titanic_voyage_ages <- sample(titanic$age[!is.na(titanic$age)], size = repetitions, replace = TRUE)
  average_age_this_voyage <- mean(titanic_voyage_ages, na.rm = TRUE)
  average_age[i] <- average_age_this_voyage
}
mean_average_age <- mean(average_age)
mean_average_age
sd_average_age <- sd(average_age)
sd_average_age

ggplot(data = tibble(average_age), aes(average_age)) +
  geom_density() +
  geom_vline(aes(xintercept = mean_average_age), color = "red") +
  geom_vline(aes(xintercept = mean_average_age+2*sd_average_age), color = "blue") +
  geom_vline(aes(xintercept = mean_average_age-2*sd_average_age), color = "blue")
```

- Each average must have the same number of elements as the original. Else it is invalid.
    - Drops NAs
- We can look at a single sample of ages.

```{r}
## These are the samepled ages of our LAST randomly selected voyage.
head(titanic_voyage_ages)
```

- So if this works, and it does, we can apply this idea on a larger scale.
- Imagine selecting a copy/sample of our titanic data (rows).
- This is like the bootstrap, but for the entire data set!
- But, we need to drop any row with a NA (null) value, because those violate the bootstrap rule!

```{r}
titanic_no_na <-
  titanic %>%
  ## We've already proven we don't want/need these.
  select(-name, -ticket, -cabin, -passengerid) %>%
  drop_na()

titanic_new_voyage <- 
  titanic_no_na %>%
  sample_n(size = nrow(titanic_no_na), replace = TRUE)

## Our original data:
titanic_no_na %>% group_by(survived) %>% count()

## Our example sampled data:
titanic_new_voyage %>% group_by(survived) %>% count()
```

- Imagine doing this a thousand times and each time, getting a slightly different sample.
- And, to make sure the trees themselves model on different features, each tree is modeled using a random subset of the features.
    - Some models will not have age or sex, for example.
    - And some models will not include passenger class.
    - This means that the models differ quite a lot.
    
```{r}
cart_model_new <- 
  titanic_new_voyage %>%
  ## This model doesn't use gender or passenger class.
  select(-sex, -pclass) %>%
  rpart(died~., data = ., method = "class")
summary(cart_model_new)
rpart.plot(cart_model_new)
```

- Imagine doing this a thousand times and then AVERAGING the risk scores.
- Because THIS is a random forest.
- Lots of decision trees, created at random, creates a random forest.
    - They are statisticians, not comedians.

Advantages:

- Typically have very good performance
- Remarkably good “out-of-the box” - very little tuning required
- Built-in validation set - don’t need to sacrifice data for extra validation
- No pre-processing required
- Robust to outliers

Disadvantages:

- Can become slow on large data sets
- Although accurate, often cannot compete with advanced boosting algorithms
- Less interpretable
- If missingness itself is a pattern, random forests will tend to miss this

```{r}
rf_model <- randomForest(died~sex+pclass+age+embarked, data = titanic_no_na)
summary(rf_model)
plot(rf_model)
```

```{r}
titanic_no_na$rf_model <-
  predict(rf_model, newdata = titanic_no_na %>% mutate(died = NA))

titanic_no_na <-
  titanic_no_na %>%
  mutate(
    ## Using a risk of .5 as the cut point is the same as using a odds of > 1.
    predicted_cart_model = case_when(rf_model > .5~1, TRUE~0)
  )

titanic_no_na %>%
  group_by(died) %>%
  summarize(
    predicted_died = sum(if_else(predicted_cart_model == 1, 1, 0)),
    predicted_survived = sum(if_else(predicted_cart_model == 0, 1, 0))
  )
```

- Accuracy =`r  (422 + 217)/nrow(titanic_no_na)` which is 82.45 percent.
- No, not a lot better, but the same features.
- This is a reasonable introduction to random forests, which are a form of black-box machine learning.

# Next Steps/Lab

- My goal with this class is to get students, like you excited about data.
- I never feel like I really succeed.
- So I'm going to do something completely different today.
- I'm going to ask you what would get you engaged/interested in data.
- And I'm going to let you choose your own adventure tonight/next week.






